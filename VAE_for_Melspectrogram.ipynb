{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE for Melspectrogram.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP3fBXxBh6hkqzXtwCmArn4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehdihosseinimoghadam/Signal-Processing/blob/main/VAE_for_Melspectrogram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader\n"
      ],
      "metadata": {
        "id": "LRUURmNK7GOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "## Some imports\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import IPython.display as idp\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "SRZcD5g_7sNV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
        "  fig, axs = plt.subplots(1, 1)\n",
        "  axs.set_title(title or 'Spectrogram (db)')\n",
        "  axs.set_ylabel(ylabel)\n",
        "  axs.set_xlabel('frame')\n",
        "  im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n",
        "  if xmax:\n",
        "    axs.set_xlim((0, xmax))\n",
        "  fig.colorbar(im, ax=axs)\n",
        "  plt.figure(figsize=(10,30))\n",
        "  plt.show(block=False)"
      ],
      "metadata": {
        "id": "EsoCC6CC-857"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (20,4)\n",
        "n_fft = 1024\n",
        "win_length = None\n",
        "hop_length = 512\n",
        "n_mels = 128\n",
        "sample_rate = 16000\n",
        "\n",
        "mel_spectrogram = T.MelSpectrogram(\n",
        "    sample_rate=sample_rate,\n",
        "    n_fft=n_fft,\n",
        "    win_length=win_length,\n",
        "    hop_length=hop_length,\n",
        "    center=True,\n",
        "    pad_mode=\"reflect\",\n",
        "    power=2.0,\n",
        "    norm='slaney',\n",
        "    onesided=True,\n",
        "    n_mels=n_mels,\n",
        "    mel_scale=\"htk\",\n",
        ")"
      ],
      "metadata": {
        "id": "95Dsk6Z_-he-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle"
      ],
      "metadata": {
        "id": "C5dqPSI0XvnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "cbIeYCMYXwMu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "qNshw2VRXzN2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "oW0URQH_X1gb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d luantm/speech-commands-tensorflow"
      ],
      "metadata": {
        "id": "ImhJCZKGYjEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!unzip /content/speech-commands-tensorflow.zip"
      ],
      "metadata": {
        "id": "qeP34r9FYlHT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader Structure\n",
        "\n",
        "- Class: EnglishCommands\n",
        "  - __init__\n",
        "  - __len__\n",
        "  - __getitem__\n",
        "   - change_sample_rate\n",
        "   - right_padding\n",
        "   - mix_down\n",
        "   - transformations\n",
        "   - cut_signal\n"
      ],
      "metadata": {
        "id": "ogV-cYoPdqE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open('/content/speechdataset/SpeechDataset/testing_list.txt', 'r') as in_file:\n",
        "    stripped = (line.strip() for line in in_file)\n",
        "    lines = (line.split(\"/\") for line in stripped if line)\n",
        "    with open('log.csv', 'w') as out_file:\n",
        "        writer = csv.writer(out_file)\n",
        "        writer.writerow(('command', 'path'))\n",
        "        writer.writerows(lines)"
      ],
      "metadata": {
        "id": "tH1ZXeE_iYsX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.cbook import get_label\n",
        "from torch.utils.data import Dataset\n",
        "import torchaudio\n",
        "class EnglishCommands(Dataset):\n",
        "  def __init__(self,\n",
        "               annotation_file,\n",
        "               audio_dir,\n",
        "               target_sample_rate,\n",
        "               transformation,\n",
        "               num_sample):\n",
        "\n",
        "\n",
        "    self.annotation_file = pd.read_csv(annotation_file).iloc[1:512,:]\n",
        "    self.audio_dir = audio_dir\n",
        "    self.target_sample_rate = target_sample_rate\n",
        "    self.transformation = transformation\n",
        "    self.num_sample = num_sample\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotation_file)\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    path = self.get_path(index)\n",
        "    label = self.get_label(index)\n",
        "    signal, sr = torchaudio.load(path)\n",
        "    signal = self.change_sample_rate(signal, sr)\n",
        "    signal = self.right_padding(signal)\n",
        "    signal = self.cut_signal(signal)\n",
        "    signal = self.transformation(signal)\n",
        "    return signal, label\n",
        "\n",
        "\n",
        "\n",
        "  def get_path(self, index):\n",
        "    path = str(self.audio_dir) + \"/\" + str(self.annotation_file.iloc[index,0]) + \"/\" + str(self.annotation_file.iloc[index,1])  \n",
        "    return path\n",
        "\n",
        "\n",
        "  def get_label(self, index):  \n",
        "    return self.annotation_file.iloc[index,0]\n",
        "\n",
        "  def change_sample_rate(self, signal, sr):\n",
        "    if sr != self.target_sample_rate:  \n",
        "       signal = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
        "       return signal  \n",
        "    return signal   \n",
        "\n",
        "  def right_padding(self, signal):\n",
        "    if signal.shape[1] < self.num_sample:\n",
        "      padd = self.num_sample - signal.shape[1] \n",
        "      last_dim_padding = (0, padd)\n",
        "      signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
        "      return signal\n",
        "    return signal  \n",
        "\n",
        "\n",
        "  def cut_signal(self, signal): \n",
        "    if signal.shape[1] > self.num_sample:\n",
        "       signal = signal[:, :self.num_sample]\n",
        "       return signal\n",
        "    return signal   \n"
      ],
      "metadata": {
        "id": "J8BWz9A6bnDH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SR = 16000\n",
        "N_FFT=1024\n",
        "HOP_LENGTH=512\n",
        "N_MELS=128\n",
        "\n",
        "melspec = torchaudio.transforms.MelSpectrogram(sample_rate = SR,\n",
        "                                               n_fft = N_FFT,\n",
        "                                               window_fn = torch.hann_window,\n",
        "                                               hop_length = HOP_LENGTH,\n",
        "                                               n_mels = N_MELS)\n",
        "\n",
        "EC = EnglishCommands(\"/content/log.csv\",\n",
        "                \"/content/speechdataset/SpeechDataset\",\n",
        "                16000,\n",
        "                melspec,\n",
        "                16000)\n",
        "\n",
        "print(len(EC))\n",
        "print(EC[6])\n",
        "print(EC[6][0].shape)\n",
        "\n",
        "plot_spectrogram(\n",
        "    EC[6][0][0], title=\"MelSpectrogram - torchaudio\", ylabel='mel freq')\n",
        "ipd.Audio(EC.get_path(6))"
      ],
      "metadata": {
        "id": "wNNLWfDm11sC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full VAE Code"
      ],
      "metadata": {
        "id": "bxc6Cbmt4sB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.activation import ReLU\n",
        "from torch.nn.modules.batchnorm import BatchNorm2d\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleConvBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, **kwargs):\n",
        "    super(SimpleConvBlock, self).__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels,\n",
        "                  out_channels,\n",
        "                  **kwargs),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU()          \n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)\n",
        "\n",
        "\n",
        "class DilatedConvBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, dilation, **kwargs):\n",
        "    super(DilatedConvBlock, self).__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels,\n",
        "                  out_channels,\n",
        "                  dilation=dilation, \n",
        "                  **kwargs),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU() \n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)\n",
        "\n",
        "\n",
        "\n",
        "class OnebyOneConvBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(OnebyOneConvBlock, self).__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, \n",
        "                  out_channels,\n",
        "                  1,\n",
        "                  1,\n",
        "                  0),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU()          \n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "\n",
        "    self.fc_mu = nn.Linear(75*8*2*out_channels, 128)\n",
        "    self.fc_logvar = nn.Linear(75*8*2*out_channels, 128)\n",
        "\n",
        "    self.simpleconv3x3 = SimpleConvBlock(self.in_channels,\n",
        "                                      self.out_channels,\n",
        "                                      kernel_size=3,\n",
        "                                      stride=1,\n",
        "                                      padding=1)\n",
        "\n",
        "    self.simpleconv5x5 = SimpleConvBlock(self.in_channels,\n",
        "                                      self.out_channels,\n",
        "                                      kernel_size=5,\n",
        "                                      stride=1,\n",
        "                                      padding=2)\n",
        "    \n",
        "    self.simpleconv7x7 = SimpleConvBlock(self.in_channels,\n",
        "                                      self.out_channels,\n",
        "                                      kernel_size=7,\n",
        "                                      stride=1,\n",
        "                                      padding=3)   \n",
        "\n",
        "\n",
        "\n",
        "    self.DilatedConvBlock2x2 = DilatedConvBlock(self.in_channels,\n",
        "                                             self.out_channels,\n",
        "                                             kernel_size = 3,\n",
        "                                             stride = 1,\n",
        "                                             padding = 2,\n",
        "                                             dilation = 2)\n",
        "\n",
        "    self.DilatedConvBlock4x4 = DilatedConvBlock(self.in_channels,\n",
        "                                             self.out_channels,\n",
        "                                             kernel_size = 3,\n",
        "                                             stride = 1,\n",
        "                                             padding = 4,\n",
        "                                             dilation = 4)\n",
        "\n",
        "\n",
        "    self.OnebyOneConvBlock = OnebyOneConvBlock(self.out_channels, \n",
        "                                               self.out_channels * 2)\n",
        "    \n",
        "\n",
        "\n",
        "    self.ConvList = nn.ModuleList()\n",
        "    for i in range(1,5):\n",
        "      self.ConvList.append(nn.Conv2d(\n",
        "              self.out_channels * 15 *i ,\n",
        "              self.out_channels * 15 *(i+1),\n",
        "              4,\n",
        "              2,\n",
        "              1 \n",
        "          ))\n",
        "      # self.ConvList.append(nn.BatchNorm2d(self.out_channels * 15 *(i+1)))\n",
        "      # self.ConvList.append(nn.ReLU())\n",
        "   \n",
        "\n",
        "  def forward(self, x):\n",
        "      # print(\"Encoder ...\")\n",
        "      x = torch.cat([self.simpleconv3x3(x),\n",
        "                     self.simpleconv5x5(x),\n",
        "                     self.simpleconv7x7(x),\n",
        "                     self.DilatedConvBlock2x2(x),\n",
        "                     self.DilatedConvBlock4x4(x),\n",
        "\n",
        "                     self.OnebyOneConvBlock(self.simpleconv3x3(x)),\n",
        "                     self.OnebyOneConvBlock(self.simpleconv5x5(x)),\n",
        "                     self.OnebyOneConvBlock(self.simpleconv7x7(x)),\n",
        "                     self.OnebyOneConvBlock(self.DilatedConvBlock2x2(x)),\n",
        "                     self.OnebyOneConvBlock(self.DilatedConvBlock4x4(x))] , axis = 1)\n",
        "      # print(x.shape)\n",
        "      for i in self.ConvList:\n",
        "        x = i(x)\n",
        "        # print(x.shape, i)\n",
        "      x = x.reshape(x.shape[0], -1)\n",
        "      # x = self.fc(x)\n",
        "      # return x \n",
        "      x_mu = self.fc_mu(x)\n",
        "      # print(x_mu.shape)\n",
        "      x_logvar = self.fc_logvar(x)\n",
        "      return x_mu, x_logvar\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, out_channels):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.fc = nn.Linear(in_features=128, out_features=75*8*2*out_channels)\n",
        "        self.conv1 = nn.ConvTranspose2d(in_channels=75*out_channels, out_channels=75, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.ConvTranspose2d(in_channels=75, out_channels=50, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = nn.ConvTranspose2d(in_channels=50, out_channels=40, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv4 = nn.ConvTranspose2d(in_channels=40, out_channels=1, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv5 = nn.ConvTranspose2d(in_channels=12, out_channels=6, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv6 = nn.ConvTranspose2d(in_channels=25, out_channels=1, kernel_size=4, stride=2, padding=1)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        # print(\"Decoder ...\")\n",
        "        x = self.fc(x)\n",
        "        x = x.view(x.size(0), 150, 8, 2) # unflatten batch of feature vectors to a batch of multi-channel feature maps\n",
        "        # print(x.shape)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        # print(x.shape)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        # print(x.shape)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        # print(x.shape)\n",
        "        # x = F.relu(self.conv4(x))\n",
        "        # print(x.shape)\n",
        "        # x = F.relu(self.conv5(x))\n",
        "        x = torch.sigmoid(self.conv4(x))\n",
        "        # print(x.shape) # last layer before output is sigmoid, since we are using BCE as reconstruction loss\n",
        "        return x\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "        self.encoder = Encoder(1,2)\n",
        "        self.decoder = Decoder(2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        latent_mu, latent_logvar = self.encoder(x)\n",
        "        latent = self.latent_sample(latent_mu, latent_logvar)\n",
        "        x_recon = self.decoder(latent)\n",
        "        return x_recon, latent_mu, latent_logvar\n",
        "    \n",
        "    def latent_sample(self, mu, logvar):\n",
        "        if self.training:\n",
        "            # the reparameterization trick\n",
        "            std = logvar.mul(0.5).exp_()\n",
        "            eps = torch.empty_like(std).normal_()\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "    \n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    # recon_x is the probability of a multivariate Bernoulli distribution p.\n",
        "    # -log(p(x)) is then the pixel-wise binary cross-entropy.\n",
        "    # Averaging or not averaging the binary cross-entropy over all pixels here\n",
        "    # is a subtle detail with big effect on training, since it changes the weight\n",
        "    # we need to pick for the other loss term by several orders of magnitude.\n",
        "    # Not averaging is the direct implementation of the negative log likelihood,\n",
        "    # but averaging makes the weight of the other loss term independent of the image resolution.\n",
        "    recon_loss = F.binary_cross_entropy(recon_x.view(-1, 1* 128* 32), x.view(-1, 1* 128* 32), reduction='sum')\n",
        "    \n",
        "    # KL-divergence between the prior distribution over latent vectors\n",
        "    # (the one we are going to sample from when generating new images)\n",
        "    # and the distribution estimated by the generator for the given image.\n",
        "    kldivergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    \n",
        "    return recon_loss + 1 * kldivergence\n",
        "    \n",
        "    \n",
        "vae = VariationalAutoencoder()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "vae = vae.to(device)\n",
        "\n",
        "num_params = sum(p.numel() for p in vae.parameters() if p.requires_grad)\n",
        "print('Number of parameters: %d' % num_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBAikB4E4rvw",
        "outputId": "a4015121-2fd4-49a7-9701-6b21010dd86c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 1775293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(EC, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "XVgE50V38YAc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train VAE"
      ],
      "metadata": {
        "id": "rB2NQhs7clW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=vae.parameters(), lr=.001, weight_decay=1e-5)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "num_epochs = 20000\n",
        "# set to training mode\n",
        "vae.train()\n",
        "\n",
        "train_loss_avg = []\n",
        "i = 0\n",
        "\n",
        "print('Training ...')\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss_avg.append(0)\n",
        "    num_batches = 0\n",
        "    \n",
        "    for image_batch, _ in train_dataloader:\n",
        "        \n",
        "        image_batch = image_batch.to(device)\n",
        "\n",
        "        # vae reconstruction\n",
        "        image_batch_recon, latent_mu, latent_logvar = vae(image_batch)\n",
        "        \n",
        "        # reconstruction error\n",
        "        loss = vae_loss(image_batch_recon, image_batch, latent_mu, latent_logvar)\n",
        "        \n",
        "        # backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # one step of the optmizer (using the gradients from backpropagation)\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss_avg[-1] += loss.item()\n",
        "        num_batches += 1\n",
        "        i+=1\n",
        "\n",
        "    if i % 50 == 0:\n",
        "      plot_spectrogram(\n",
        "    image_batch_recon[0][0].detach().numpy(), title=\"MelSpectrogram - torchaudio\", ylabel='mel freq')    \n",
        "        \n",
        "    train_loss_avg[-1] /= num_batches\n",
        "    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))"
      ],
      "metadata": {
        "id": "re1Paudl9HTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aXE0x9loe8xH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}